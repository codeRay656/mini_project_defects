# -*- coding: utf-8 -*-
"""mini_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19QoU31zs-M_Ap0OzlzUAozQTQwt4t9Ln

1. Load the Required Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import classification_report, confusion_matrix

"""Load DEFECT Data"""

from google.colab import drive
drive.mount('/content/gdrive')
csvfile = '/content/gdrive/MyDrive/ML OPS/defects_data.csv'
df=pd.read_csv(csvfile)
df = df.dropna()
df.head()

"""Check the statistics"""

df.describe()

# Feature matrix (X) - choose relevant columns
X = df[['severity', 'inspection_method', 'defect_location', 'repair_cost']]
X = pd.get_dummies(X, drop_first=True)

# Target vector (y) - the label to classify
y = df['defect_type']

"""Use scaler to have the data in the normal range"""

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

"""Label Encoding for target variable"""

le = LabelEncoder()
y_encoded = le.fit_transform(y)

"""Convert to one hot encoding"""

y_one_hot = to_categorical(y_encoded, num_classes=3)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_one_hot, test_size=0.2, random_state=42)

print(X_train.shape[1])  # Should print 7
model = Sequential()
model.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(len(set(y)), activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

mlp = model.fit(X_train, y_train, epochs=100, batch_size=5, validation_data=(X_test, y_test))

loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy:.4f}")

import matplotlib.pyplot as plt
plt.plot(mlp.history['accuracy'], label='Train Accuracy')
plt.plot(mlp.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Model Accuracy vs. Epoch')
plt.legend()
plt.show()

y_pred_probs = model.predict(X_test)  #  probability distributions
y_pred_classes = np.argmax(y_pred_probs, axis=1)  # Convert to class labels

# Convert y_test from one-hot to class labels
y_test_classes = np.argmax(y_test, axis=1)

print("\nClassification Report:")
print(classification_report(y_test_classes, y_pred_classes, target_names=le.classes_))

cm = confusion_matrix(y_test_classes, y_pred_classes)

plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

print("Confusion Matrix:\n", cm)

"""Saving as .h5 *file*"""

import joblib
from tensorflow.keras.models import load_model

model.save("mlp_model.h5")

"""Predict the value of Xtest"""

from tensorflow.keras.models import load_model
import numpy as np

loaded_model = load_model("mlp_model.h5")

y_pred_probs = loaded_model.predict(X_test)  # Probability distributions
y_pred_classes = np.argmax(y_pred_probs, axis=1)
print(y_test_classes)
print(y_pred_classes)

y_pred_class = np.argmax(y_pred_probs, axis=1)
y_pred_class

print(f"Predicted class: {y_pred_class[0]}")

pip install streamlit

# After model.fit()

# Save the trained model
model.save('mlp_model.h5')

# Save scaler
import pickle
with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)

# Save label encoder
with open('label_encoder.pkl', 'wb') as f:
    pickle.dump(le, f)

import streamlit as st
import numpy as np
import pickle
from tensorflow.keras.models import load_model

# Load model, scaler and label encoder
model = load_model('mlp_model.h5')

with open('scaler.pkl', 'rb') as f:
    scaler = pickle.load(f)

with open('label_encoder.pkl', 'rb') as f:
    label_encoder = pickle.load(f)

# Define inspection methods and defect locations (same as training)
inspection_methods = ['Method_A', 'Method_B', 'Method_C']  # Update as per your dataset
defect_locations = ['Location_X', 'Location_Y', 'Location_Z']  # Update as per your dataset

def preprocess_input(severity, inspection_method, defect_location, repair_cost):
    # One-hot encoding manually
    inspection_encoded = [1 if inspection_method == m else 0 for m in inspection_methods[1:]]  # drop_first=True
    location_encoded = [1 if defect_location == l else 0 for l in defect_locations[1:]]

    raw_features = [severity, repair_cost] + inspection_encoded + location_encoded
    scaled_features = scaler.transform([raw_features])

    return scaled_features

# Streamlit app
def main():
    st.set_page_config(page_title="Defect Type Predictor", page_icon="ðŸ”§", layout="centered")

    st.title("ðŸ”§ Defect Type Prediction")
    st.write("Predict the type of defect based on inspection data using a Deep Learning model.")

    severity = st.number_input("Severity", min_value=0, max_value=10, step=1)
    inspection_method = st.selectbox("Inspection Method", inspection_methods)
    defect_location = st.selectbox("Defect Location", defect_locations)
    repair_cost = st.number_input("Repair Cost", min_value=0.0, step=0.01)

    if st.button("Predict Defect Type"):
        processed_input = preprocess_input(severity, inspection_method, defect_location, repair_cost)
        prediction = model.predict(processed_input)
        predicted_class = np.argmax(prediction)
        defect_type = label_encoder.inverse_transform([predicted_class])[0]

        st.success(f"ðŸ›  Predicted Defect Type: **{defect_type}**")

    if st.checkbox("Show Preprocessed Data"):
        st.write(preprocess_input(severity, inspection_method, defect_location, repair_cost))

if __name__ == "__main__":
    main()

from google.colab import files

files.download('mlp_model.h5')
files.download('scaler.pkl')
files.download('label_encoder.pkl')

# Commented out IPython magic to ensure Python compatibility.
# %%writefile requirements.txt
# streamlit
# numpy
# pandas
# scikit-learn
# tensorflow
# pickle5
#

from google.colab import files
files.download('requirements.txt')
